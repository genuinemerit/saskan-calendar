# PR-003a: Schema Refactoring for Macro-Scale Simulation

**Status:** Approved - Ready for Implementation
**Date:** 2025-12-30
**Related ADRs:** ADR-001 (SQLite), ADR-002 (Time-Series Snapshots), ADR-008 (Meta_data Management)
**Depends On:** PR-002 (CLI and Data Operations)
**Followed By:** PR-003b (Macro-Scale Simulation Engine)

---

## Overview

Extend the timeline database schema to support macro-scale demographic simulation at the regional and provincial levels, particularly for pre-settlement eras. This provides the foundation for simulating large-scale population dynamics before specific settlements existed.

**Goal:** Enable tracking of population demographics at regional/provincial granularity with variable snapshot frequencies and query interpolation capabilities.

---

## Context

The Saskan Lands timeline begins long before the first settlements. To simulate pre-settlement demographics (migrations, proto-populations, early demographic trends), we need:

1. **Narrative context** for regions, provinces, and epochs (description fields)
2. **Flexible metadata** for regions and provinces (meta_data JSON fields)
3. **Time-series snapshots** at regional/provincial levels (parallel to settlement_snapshots)
4. **Snapshot provenance tracking** (how was this snapshot created?)
5. **Temporal granularity tracking** (year-level vs decade-level vs event-driven snapshots)
6. **Query interpolation** (estimate values between sparse snapshots)

---

## Scope

### In Scope

**Schema Changes:**
- Add `description` and `meta_data` to regions, provinces, epochs
- Create `region_snapshots` table with type/granularity tracking
- Create `province_snapshots` table with type/granularity tracking
- **Retrofit `settlement_snapshots`** with `snapshot_type` and `granularity` fields (breaking change)

**Models:**
- Update Region, Province, Epoch models (add DescriptionMixin, MetadataMixin per ADR-008)
- Create RegionSnapshot, ProvinceSnapshot models (with MetadataMixin)
- Update SettlementSnapshot model (add snapshot_type, granularity fields)

**Services:**
- RegionSnapshotService, ProvinceSnapshotService
- Interpolation utilities for sparse snapshot queries

**CLI Commands:**
- Add snapshot commands for regions/provinces
- Query flags: `--nearest`, `--interpolate`
- List/export support for new snapshot types

**Testing:**
- Unit tests for interpolation logic
- Integration tests for snapshot CRUD
- Query behavior tests (nearest vs interpolated)

**Documentation:**
- Update README.md with new capabilities
- Update TECHNICAL.md with schema documentation
- Add usage examples

### Out of Scope (Deferred)

- **Meta_data field updates via CLI** (separate PR after ADR-008 implementation complete)
- **Description field updates via CLI** (separate PR after ADR-008 implementation complete)
- **Simulation engine** (PR-003b)
- **Event system** (future PR)
- **pulse_offset field** (temporal precision - future PR)

### ADR-008 Integration

**In Scope for PR-003a:**
- Implement DescriptionMixin and MetadataMixin in base.py (per ADR-008 specification)
- Apply mixins to Region, Province, Epoch, and all snapshot models
- Flat structure validation for metadata (enforced at model layer)
- Programmatic metadata operations available (merge, remove, clear)
- Tests for metadata validation (nested objects/arrays rejected)

**Deferred to Later PR:**
- CLI commands for metadata updates (--meta-set, --meta-remove, etc.)
- CLI commands for description updates (--description, --clear-description)
- MetadataRepository class for database-level operations

---

## Design Decisions

### 1. Snapshot Provenance Tracking

**Decision:** Add explicit `snapshot_type` field to track how snapshots were created.

**Valid snapshot_type values:**
- `census` - Authoritative user-entered data (historical record)
- `simulation` - Generated by simulation engine
- `estimate` - User-entered estimate or educated guess
- `interpolated` - Computed value between snapshots (query-time only, not stored)
- `extrapolated` - Projected forward/backward from known snapshots

**Rationale:**
- Distinguishes authoritative data from derived values
- Enables filtering queries (e.g., "show only census data")
- Supports validation and confidence scoring
- Makes data lineage transparent

### 2. Temporal Granularity Tracking

**Decision:** Add explicit `granularity` field to track snapshot resolution.

**Valid granularity values:**
- `year` - Annual snapshot
- `decade` - Decadal snapshot (represents ~10-turn period)
- `century` - Centennial snapshot (represents ~100-turn period)
- `event-driven` - Snapshot tied to specific event (e.g., Great Migration)
- `custom` - User-defined interval

**Rationale:**
- Pre-settlement simulation uses coarser time steps
- Settlement-era simulation uses finer time steps
- Query logic can adapt to granularity (wider interpolation windows for coarse data)
- Visualization can adjust detail level based on granularity

### 3. Query Interpolation Strategy

**Decision:** Provide user choice between nearest snapshot and interpolated value.

**CLI Flags:**
```bash
# Default: Return nearest snapshot before query date
saskan-timeline list region-snapshots --region 1 --day 5000

# Explicit: Return interpolated value
saskan-timeline list region-snapshots --region 1 --day 5000 --interpolate
```

**Interpolation Method:**
- Linear interpolation for population_total
- Species/habitat breakdowns: Linear interpolation per component
- **JSON fields (cultural_composition, economic_data): Use nearest snapshot (no interpolation)** ✅ User Decision

**Edge Cases:**
- Only one snapshot exists: Return that snapshot with warning
- Query before first snapshot: Error (no data to interpolate from)
- Query after last snapshot: Return last snapshot with warning (no extrapolation by default)

**Rationale:**
- Transparency: User knows if value is observed vs computed
- Flexibility: Supports both precise queries and exploratory analysis
- Safety: Default behavior (nearest) never invents data

### 4. Meta_data and Description Fields (ADR-008 Integration)

**Decision:** Add fields and model-layer functionality now; defer CLI commands to separate PR.

**Why add now:**
- Simulation engine (PR-003b) will need to read meta_data for parameters
- Manual data entry (via JSON import) can populate these fields immediately
- Schema breaking change best done once
- Programmatic API (mixins) enables metadata management without CLI
- Validation layer (flat structure enforcement) prevents data quality issues

**Why defer CLI commands:**
- ADR-008 defines complete CLI interface (--meta-set, --meta-remove, etc.)
- Better to implement CLI consistently across all entities in one PR
- Model layer functionality more urgent than CLI convenience
- Gives time to validate mixin design with programmatic usage first

**Implementation in PR-003a:**
- Add `DescriptionMixin` and `MetadataMixin` to `base.py` (per ADR-008 spec)
- Apply mixins to Region, Province, Epoch, all snapshot models
- Flat structure validation enforced at model layer
- Tests for metadata operations and validation

---

## Schema Changes

### SQL DDL

```sql
-- ============================================================================
-- ADD DESCRIPTION AND META_DATA TO EXISTING TABLES
-- ============================================================================

-- Regions: Add description and meta_data
ALTER TABLE regions ADD COLUMN description TEXT;
ALTER TABLE regions ADD COLUMN meta_data JSON;

-- Provinces: Add description and meta_data
ALTER TABLE provinces ADD COLUMN description TEXT;
ALTER TABLE provinces ADD COLUMN meta_data JSON;

-- Epochs: Add description
ALTER TABLE epochs ADD COLUMN description TEXT;

-- ============================================================================
-- RETROFIT SETTLEMENT_SNAPSHOTS (Breaking Change)
-- ============================================================================

-- Add snapshot_type and granularity for consistency with regional/provincial snapshots
ALTER TABLE settlement_snapshots ADD COLUMN snapshot_type VARCHAR NOT NULL DEFAULT 'simulation';
ALTER TABLE settlement_snapshots ADD COLUMN granularity VARCHAR NOT NULL DEFAULT 'year';

-- Create indexes for new fields
CREATE INDEX idx_settlement_snapshots_type ON settlement_snapshots(snapshot_type);
CREATE INDEX idx_settlement_snapshots_granularity ON settlement_snapshots(granularity);

-- ============================================================================
-- NEW TABLE: REGION_SNAPSHOTS
-- ============================================================================

CREATE TABLE region_snapshots (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    region_id INTEGER NOT NULL,
    astro_day INTEGER NOT NULL,

    -- Provenance and granularity tracking
    snapshot_type VARCHAR NOT NULL DEFAULT 'simulation',
    granularity VARCHAR NOT NULL DEFAULT 'year',

    -- Core demographic data
    population_total INTEGER NOT NULL,
    population_by_species JSON,      -- {"huum": 10000, "sint": 2000, ...}
    population_by_habitat JSON,      -- {"urban": 5000, "rural": 7000, ...}

    -- Cultural and economic data
    cultural_composition JSON,       -- {"languages": {...}, "religions": {...}, ...}
    economic_data JSON,              -- {"industries": [...], "trade_goods": [...], ...}

    -- Flexible extension
    meta_data JSON,

    -- Metadata
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,

    -- Constraints
    FOREIGN KEY (region_id) REFERENCES regions(id),

    -- Indexes
    CHECK (population_total >= 0),
    CHECK (astro_day >= 0)
);

-- Indexes for region_snapshots
CREATE INDEX idx_region_snapshots_region_day ON region_snapshots(region_id, astro_day);
CREATE INDEX idx_region_snapshots_day ON region_snapshots(astro_day);
CREATE INDEX idx_region_snapshots_type ON region_snapshots(snapshot_type);
CREATE INDEX idx_region_snapshots_granularity ON region_snapshots(granularity);

-- ============================================================================
-- NEW TABLE: PROVINCE_SNAPSHOTS
-- ============================================================================

CREATE TABLE province_snapshots (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    province_id INTEGER NOT NULL,
    astro_day INTEGER NOT NULL,

    -- Provenance and granularity tracking
    snapshot_type VARCHAR NOT NULL DEFAULT 'simulation',
    granularity VARCHAR NOT NULL DEFAULT 'year',

    -- Core demographic data
    population_total INTEGER NOT NULL,
    population_by_species JSON,
    population_by_habitat JSON,

    -- Cultural and economic data
    cultural_composition JSON,
    economic_data JSON,

    -- Flexible extension
    meta_data JSON,

    -- Metadata
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,

    -- Constraints
    FOREIGN KEY (province_id) REFERENCES provinces(id),

    -- Indexes
    CHECK (population_total >= 0),
    CHECK (astro_day >= 0)
);

-- Indexes for province_snapshots
CREATE INDEX idx_province_snapshots_province_day ON province_snapshots(province_id, astro_day);
CREATE INDEX idx_province_snapshots_day ON province_snapshots(astro_day);
CREATE INDEX idx_province_snapshots_type ON province_snapshots(snapshot_type);
CREATE INDEX idx_province_snapshots_granularity ON province_snapshots(granularity);
```

### Migration Strategy

**Option A: Drop and Recreate (Development Phase)**

Since we're still in early development (v0.2.0 → v0.3.0) and likely have minimal production data:

```bash
# Backup existing data
saskan-timeline io export backup_v0.2.0.json --include-inactive

# Drop database
saskan-timeline db drop --yes

# Reinitialize with new schema
saskan-timeline db init

# Reimport data (regions/provinces will have NULL description/meta_data)
saskan-timeline io import backup_v0.2.0.json
```

**Option B: ALTER TABLE (Production Safe)**

For future schema changes when production data exists:

```python
# In db/schema.py or migration script
def migrate_v0_2_to_v0_3(engine):
    """Migrate database from v0.2.0 to v0.3.0."""
    with engine.begin() as conn:
        # Add new columns (NULL allowed for backward compatibility)
        conn.execute(text("ALTER TABLE regions ADD COLUMN description TEXT"))
        conn.execute(text("ALTER TABLE regions ADD COLUMN meta_data JSON"))
        conn.execute(text("ALTER TABLE provinces ADD COLUMN description TEXT"))
        conn.execute(text("ALTER TABLE provinces ADD COLUMN meta_data JSON"))
        conn.execute(text("ALTER TABLE epochs ADD COLUMN description TEXT"))

        # Create new tables
        Base.metadata.tables['region_snapshots'].create(conn)
        Base.metadata.tables['province_snapshots'].create(conn)

        # Update version in metadata table (if we had one)
        # conn.execute(text("UPDATE schema_version SET version = '0.3.0'"))
```

**Recommendation for PR-003a:** Use Option A (drop/recreate). Add migration tooling later when schema stabilizes.

---

## Model Updates

### Base Mixins (ADR-008 Implementation)

**Add to `app_timeline/models/base.py`:**

```python
from typing import Dict, List, Optional, Any

class DescriptionMixin:
    """Mixin for description field management (ADR-008)."""

    def update_description(self, text: Optional[str]) -> Optional[str]:
        """Update description text."""
        self.description = text
        return self.description

    def clear_description(self) -> None:
        """Remove description (set to NULL)."""
        self.description = None

    def has_description(self) -> bool:
        """Check if entity has a description."""
        return bool(self.description)


class MetadataMixin:
    """Mixin for metadata field management with flat structure validation (ADR-008)."""

    @staticmethod
    def _validate_flat_structure(data: Dict[str, Any]) -> None:
        """Validate that metadata is a flat key-value structure (no nesting/arrays)."""
        for key, value in data.items():
            if isinstance(value, dict):
                raise ValueError(
                    f"Nested objects not allowed in metadata. Key '{key}' contains object."
                )
            if isinstance(value, (list, tuple)):
                raise ValueError(
                    f"Arrays not allowed in metadata. Key '{key}' contains array."
                )
            if not isinstance(value, (str, int, float, bool, type(None))):
                raise ValueError(
                    f"Invalid value type for key '{key}': {type(value).__name__}. "
                    f"Only str, int, float, bool, None allowed."
                )

    def update_metadata(
        self,
        updates: Dict[str, Any],
        mode: str = 'merge'
    ) -> Dict[str, Any]:
        """Update metadata with specified mode ('merge' or 'replace')."""
        if mode not in ('merge', 'replace'):
            raise ValueError(f"Invalid mode: {mode}. Use 'merge' or 'replace'")

        self._validate_flat_structure(updates)

        if mode == 'replace':
            self.meta_data = updates
        else:  # merge
            current = self.meta_data or {}
            current.update(updates)
            self.meta_data = current

        return self.meta_data

    def merge_metadata(self, updates: Dict[str, Any]) -> Dict[str, Any]:
        """Convenience method: merge updates into existing metadata."""
        return self.update_metadata(updates, mode='merge')

    def remove_metadata_keys(self, keys: List[str]) -> Dict[str, Any]:
        """Remove specific keys from metadata."""
        if not self.meta_data:
            return {}
        for key in keys:
            self.meta_data.pop(key, None)
        return self.meta_data

    def clear_metadata(self) -> None:
        """Remove all metadata."""
        self.meta_data = None

    def get_metadata_value(self, key: str, default: Any = None) -> Any:
        """Get a specific metadata value with optional default."""
        if not self.meta_data:
            return default
        return self.meta_data.get(key, default)

    def has_metadata_key(self, key: str) -> bool:
        """Check if a metadata key exists."""
        return bool(self.meta_data and key in self.meta_data)
```

### Updated Models

**Region Model** (`app_timeline/models/province.py`)

```python
from app_timeline.models.base import (
    Base, PrimaryKeyMixin, TimestampMixin, TemporalBoundsMixin,
    DescriptionMixin, MetadataMixin  # ADR-008
)

class Region(Base, PrimaryKeyMixin, TimestampMixin, TemporalBoundsMixin,
             DescriptionMixin, MetadataMixin):  # ADR-008 mixins
    __tablename__ = "regions"

    name = Column(String, unique=True, nullable=False, index=True)
    description = Column(Text, nullable=True)
    is_active = Column(Boolean, nullable=False, default=True, index=True)
    meta_data = Column(JSON, nullable=True)

    # Relationships
    provinces = relationship("Province", back_populates="region")
    snapshots = relationship("RegionSnapshot", back_populates="region")
```

**Province Model** (`app_timeline/models/province.py`)

```python
class Province(Base, PrimaryKeyMixin, TimestampMixin, TemporalBoundsMixin,
               DescriptionMixin, MetadataMixin):  # ADR-008 mixins
    __tablename__ = "provinces"

    name = Column(String, unique=True, nullable=False, index=True)
    description = Column(Text, nullable=True)
    region_id = Column(Integer, ForeignKey("regions.id"), nullable=True, index=True)
    is_active = Column(Boolean, nullable=False, default=True, index=True)
    meta_data = Column(JSON, nullable=True)

    # Relationships
    region = relationship("Region", back_populates="provinces")
    settlements = relationship("Settlement", back_populates="province")
    snapshots = relationship("ProvinceSnapshot", back_populates="province")
```

**Epoch Model** (`app_timeline/models/epoch.py`)

```python
class Epoch(Base, PrimaryKeyMixin, TimestampMixin,
            DescriptionMixin):  # ADR-008 mixin (no metadata for epochs)
    __tablename__ = "epochs"

    name = Column(String, unique=True, nullable=False, index=True)
    description = Column(Text, nullable=True)
    start_astro_day = Column(Integer, nullable=False, index=True)
    end_astro_day = Column(Integer, nullable=False, index=True)
```

**SettlementSnapshot Model** (`app_timeline/models/settlement.py`) - **UPDATED**

```python
class SettlementSnapshot(Base, PrimaryKeyMixin, TimestampMixin, MetadataMixin):  # ADR-008
    __tablename__ = "settlement_snapshots"

    settlement_id = Column(Integer, ForeignKey("settlements.id"), nullable=False, index=True)
    astro_day = Column(Integer, nullable=False, index=True)

    # NEW: Provenance and granularity (consistency with region/province snapshots)
    snapshot_type = Column(String, nullable=False, default="simulation", index=True)
    granularity = Column(String, nullable=False, default="year", index=True)

    # Demographics (existing fields)
    population_total = Column(Integer, nullable=False)
    population_by_species = Column(JSON, nullable=True)
    # ... rest of fields
```

### New Models

**RegionSnapshot Model** (`app_timeline/models/region_snapshot.py`)

```python
from datetime import datetime, timezone
from sqlalchemy import Column, Integer, String, JSON, DateTime, ForeignKey, CheckConstraint
from sqlalchemy.orm import relationship
from app_timeline.models.base import Base, PrimaryKeyMixin, TimestampMixin, MetadataMixin  # ADR-008

class RegionSnapshot(Base, PrimaryKeyMixin, TimestampMixin, MetadataMixin):  # ADR-008
    """Time-series demographic snapshot for a region."""

    __tablename__ = "region_snapshots"

    # Foreign keys
    region_id = Column(Integer, ForeignKey("regions.id"), nullable=False, index=True)

    # Temporal
    astro_day = Column(Integer, nullable=False, index=True)

    # Provenance
    snapshot_type = Column(String, nullable=False, default="simulation", index=True)
    granularity = Column(String, nullable=False, default="year", index=True)

    # Demographics
    population_total = Column(Integer, nullable=False)
    population_by_species = Column(JSON, nullable=True)
    population_by_habitat = Column(JSON, nullable=True)

    # Culture and economy
    cultural_composition = Column(JSON, nullable=True)
    economic_data = Column(JSON, nullable=True)

    # Extension
    meta_data = Column(JSON, nullable=True)

    # Relationships
    region = relationship("Region", back_populates="snapshots")

    # Constraints
    __table_args__ = (
        CheckConstraint("population_total >= 0", name="check_region_snapshot_population_positive"),
        CheckConstraint("astro_day >= 0", name="check_region_snapshot_astro_day_nonnegative"),
    )

    def __repr__(self):
        return (
            f"<RegionSnapshot(id={self.id}, region_id={self.region_id}, "
            f"astro_day={self.astro_day}, population={self.population_total}, "
            f"type={self.snapshot_type}, granularity={self.granularity})>"
        )
```

**ProvinceSnapshot Model** (`app_timeline/models/province_snapshot.py`)

```python
from datetime import datetime, timezone
from sqlalchemy import Column, Integer, String, JSON, DateTime, ForeignKey, CheckConstraint
from sqlalchemy.orm import relationship
from app_timeline.models.base import Base, PrimaryKeyMixin, TimestampMixin, MetadataMixin  # ADR-008

class ProvinceSnapshot(Base, PrimaryKeyMixin, TimestampMixin, MetadataMixin):  # ADR-008
    """Time-series demographic snapshot for a province."""

    __tablename__ = "province_snapshots"

    # Foreign keys
    province_id = Column(Integer, ForeignKey("provinces.id"), nullable=False, index=True)

    # Temporal
    astro_day = Column(Integer, nullable=False, index=True)

    # Provenance
    snapshot_type = Column(String, nullable=False, default="simulation", index=True)
    granularity = Column(String, nullable=False, default="year", index=True)

    # Demographics
    population_total = Column(Integer, nullable=False)
    population_by_species = Column(JSON, nullable=True)
    population_by_habitat = Column(JSON, nullable=True)

    # Culture and economy
    cultural_composition = Column(JSON, nullable=True)
    economic_data = Column(JSON, nullable=True)

    # Extension
    meta_data = Column(JSON, nullable=True)

    # Relationships
    province = relationship("Province", back_populates="snapshots")

    # Constraints
    __table_args__ = (
        CheckConstraint("population_total >= 0", name="check_province_snapshot_population_positive"),
        CheckConstraint("astro_day >= 0", name="check_province_snapshot_astro_day_nonnegative"),
    )

    def __repr__(self):
        return (
            f"<ProvinceSnapshot(id={self.id}, province_id={self.province_id}, "
            f"astro_day={self.astro_day}, population={self.population_total}, "
            f"type={self.snapshot_type}, granularity={self.granularity})>"
        )
```

---

## Service Layer

### New Services

**RegionSnapshotService** (`app_timeline/services/region_snapshot_service.py`)

```python
from typing import List, Optional, Dict, Any, Tuple
from sqlalchemy import and_
from app_timeline.models import RegionSnapshot, Region
from app_timeline.services.base_service import BaseService
from app_timeline.utils.validation import validate_positive, validate_non_negative

class RegionSnapshotService(BaseService):
    """Service for managing region snapshots."""

    def create_snapshot(
        self,
        region_id: int,
        astro_day: int,
        population_total: int,
        snapshot_type: str = "simulation",
        granularity: str = "year",
        population_by_species: Optional[Dict] = None,
        population_by_habitat: Optional[Dict] = None,
        cultural_composition: Optional[Dict] = None,
        economic_data: Optional[Dict] = None,
        meta_data: Optional[Dict] = None,
    ) -> RegionSnapshot:
        """Create a new region snapshot."""
        # Validation
        validate_non_negative(astro_day, "astro_day")
        validate_non_negative(population_total, "population_total")

        # Verify region exists
        region = self.session.query(Region).filter(Region.id == region_id).first()
        if not region:
            raise ValueError(f"Region with id {region_id} not found")

        snapshot = RegionSnapshot(
            region_id=region_id,
            astro_day=astro_day,
            snapshot_type=snapshot_type,
            granularity=granularity,
            population_total=population_total,
            population_by_species=population_by_species,
            population_by_habitat=population_by_habitat,
            cultural_composition=cultural_composition,
            economic_data=economic_data,
            meta_data=meta_data,
        )

        self.session.add(snapshot)
        self.session.commit()
        self.session.refresh(snapshot)
        return snapshot

    def get_by_id(self, snapshot_id: int) -> Optional[RegionSnapshot]:
        """Get snapshot by ID."""
        return self.session.query(RegionSnapshot).filter(
            RegionSnapshot.id == snapshot_id
        ).first()

    def list_by_region(
        self,
        region_id: int,
        start_day: Optional[int] = None,
        end_day: Optional[int] = None,
        snapshot_type: Optional[str] = None,
        granularity: Optional[str] = None,
    ) -> List[RegionSnapshot]:
        """List snapshots for a region with optional filtering."""
        query = self.session.query(RegionSnapshot).filter(
            RegionSnapshot.region_id == region_id
        )

        if start_day is not None:
            query = query.filter(RegionSnapshot.astro_day >= start_day)
        if end_day is not None:
            query = query.filter(RegionSnapshot.astro_day <= end_day)
        if snapshot_type is not None:
            query = query.filter(RegionSnapshot.snapshot_type == snapshot_type)
        if granularity is not None:
            query = query.filter(RegionSnapshot.granularity == granularity)

        return query.order_by(RegionSnapshot.astro_day).all()

    def get_nearest(
        self,
        region_id: int,
        astro_day: int,
        before: bool = True
    ) -> Optional[RegionSnapshot]:
        """Get nearest snapshot to a given day.

        Args:
            region_id: Region ID
            astro_day: Target day
            before: If True, find nearest before target; if False, find nearest after
        """
        query = self.session.query(RegionSnapshot).filter(
            RegionSnapshot.region_id == region_id
        )

        if before:
            query = query.filter(RegionSnapshot.astro_day <= astro_day)
            query = query.order_by(RegionSnapshot.astro_day.desc())
        else:
            query = query.filter(RegionSnapshot.astro_day >= astro_day)
            query = query.order_by(RegionSnapshot.astro_day.asc())

        return query.first()

    def get_interpolated(
        self,
        region_id: int,
        astro_day: int
    ) -> Optional[Dict[str, Any]]:
        """Get interpolated snapshot data for a given day.

        Returns a dict with interpolated values, not a RegionSnapshot object.
        """
        # Get nearest snapshots before and after
        before = self.get_nearest(region_id, astro_day, before=True)
        after = self.get_nearest(region_id, astro_day, before=False)

        # Edge cases
        if not before and not after:
            return None  # No data available
        if not before:
            return self._snapshot_to_dict(after)  # Before first snapshot
        if not after:
            return self._snapshot_to_dict(before)  # After last snapshot
        if before.id == after.id:
            return self._snapshot_to_dict(before)  # Exact match

        # Linear interpolation
        t = (astro_day - before.astro_day) / (after.astro_day - before.astro_day)

        interpolated = {
            "region_id": region_id,
            "astro_day": astro_day,
            "snapshot_type": "interpolated",
            "granularity": before.granularity,
            "population_total": int(
                before.population_total + t * (after.population_total - before.population_total)
            ),
            "interpolation_info": {
                "before_day": before.astro_day,
                "after_day": after.astro_day,
                "before_id": before.id,
                "after_id": after.id,
            }
        }

        # Interpolate species breakdown if both have it
        if before.population_by_species and after.population_by_species:
            interpolated["population_by_species"] = self._interpolate_dict(
                before.population_by_species,
                after.population_by_species,
                t
            )
        else:
            interpolated["population_by_species"] = before.population_by_species

        # Interpolate habitat breakdown if both have it
        if before.population_by_habitat and after.population_by_habitat:
            interpolated["population_by_habitat"] = self._interpolate_dict(
                before.population_by_habitat,
                after.population_by_habitat,
                t
            )
        else:
            interpolated["population_by_habitat"] = before.population_by_habitat

        # For JSON fields, use nearest (before)
        interpolated["cultural_composition"] = before.cultural_composition
        interpolated["economic_data"] = before.economic_data
        interpolated["meta_data"] = before.meta_data

        return interpolated

    def update(self, snapshot_id: int, **kwargs) -> RegionSnapshot:
        """Update a region snapshot."""
        snapshot = self.get_by_id(snapshot_id)
        if not snapshot:
            raise ValueError(f"RegionSnapshot with id {snapshot_id} not found")

        # Update fields
        for key, value in kwargs.items():
            if hasattr(snapshot, key):
                setattr(snapshot, key, value)

        self.session.commit()
        self.session.refresh(snapshot)
        return snapshot

    def delete(self, snapshot_id: int, hard: bool = True) -> bool:
        """Delete a region snapshot (always hard delete)."""
        snapshot = self.get_by_id(snapshot_id)
        if not snapshot:
            return False

        self.session.delete(snapshot)
        self.session.commit()
        return True

    # Helper methods

    def _snapshot_to_dict(self, snapshot: RegionSnapshot) -> Dict[str, Any]:
        """Convert snapshot to dict."""
        return {
            "id": snapshot.id,
            "region_id": snapshot.region_id,
            "astro_day": snapshot.astro_day,
            "snapshot_type": snapshot.snapshot_type,
            "granularity": snapshot.granularity,
            "population_total": snapshot.population_total,
            "population_by_species": snapshot.population_by_species,
            "population_by_habitat": snapshot.population_by_habitat,
            "cultural_composition": snapshot.cultural_composition,
            "economic_data": snapshot.economic_data,
            "meta_data": snapshot.meta_data,
        }

    def _interpolate_dict(
        self,
        dict_before: Dict[str, int],
        dict_after: Dict[str, int],
        t: float
    ) -> Dict[str, int]:
        """Linearly interpolate between two dicts with numeric values."""
        result = {}
        all_keys = set(dict_before.keys()) | set(dict_after.keys())

        for key in all_keys:
            val_before = dict_before.get(key, 0)
            val_after = dict_after.get(key, 0)
            result[key] = int(val_before + t * (val_after - val_before))

        return result
```

**ProvinceSnapshotService** (`app_timeline/services/province_snapshot_service.py`)

Similar structure to RegionSnapshotService, with `province_id` instead of `region_id`.

---

## CLI Commands

### New Commands in `data` Group

**Add Region Snapshot** (`cli_data.py`)

```python
@data_app.command("add-region-snapshot")
def add_region_snapshot(
    region: int = typer.Option(..., "--region", "-r", help="Region ID"),
    day: int = typer.Option(..., "--day", "-d", help="Astro day"),
    population: int = typer.Option(..., "--population", "-p", help="Total population"),
    snapshot_type: str = typer.Option("simulation", "--type", "-t", help="Snapshot type"),
    granularity: str = typer.Option("year", "--granularity", "-g", help="Temporal granularity"),
    species: Optional[str] = typer.Option(None, "--species", help="Species breakdown (JSON)"),
    habitat: Optional[str] = typer.Option(None, "--habitat", help="Habitat breakdown (JSON)"),
    meta: Optional[List[str]] = typer.Option(None, "--meta", "-m", help="Metadata (key:value pairs)"),
):
    """Add a demographic snapshot for a region."""
    # Implementation using RegionSnapshotService
```

**Add Province Snapshot** (`cli_data.py`)

Similar to add-region-snapshot, with `--province` instead of `--region`.

### New Commands in `list` Group

**List Region Snapshots** (`cli_list.py`)

```python
@list_app.command("region-snapshots")
def list_region_snapshots(
    region: Optional[int] = typer.Option(None, "--region", "-r", help="Filter by region ID"),
    start_day: Optional[int] = typer.Option(None, "--start-day", help="Filter by start day"),
    end_day: Optional[int] = typer.Option(None, "--end-day", help="Filter by end day"),
    snapshot_type: Optional[str] = typer.Option(None, "--type", "-t", help="Filter by snapshot type"),
    granularity: Optional[str] = typer.Option(None, "--granularity", "-g", help="Filter by granularity"),
    day: Optional[int] = typer.Option(None, "--day", "-d", help="Query specific day (with --nearest or --interpolate)"),
    nearest: bool = typer.Option(False, "--nearest", help="Return nearest snapshot to --day"),
    interpolate: bool = typer.Option(False, "--interpolate", help="Return interpolated value for --day"),
):
    """List region snapshots with optional filtering and interpolation."""
    # Implementation using RegionSnapshotService
```

**List Province Snapshots** (`cli_list.py`)

Similar structure to list-region-snapshots.

### New Commands in `update` Group

**Update Region Snapshot** (`cli_update.py`)

```python
@update_app.command("region-snapshot")
def update_region_snapshot(
    snapshot_id: int = typer.Argument(..., help="Snapshot ID"),
    population: Optional[int] = typer.Option(None, "--population", "-p", help="Update population"),
    snapshot_type: Optional[str] = typer.Option(None, "--type", "-t", help="Update snapshot type"),
    granularity: Optional[str] = typer.Option(None, "--granularity", "-g", help="Update granularity"),
    # ... other fields
):
    """Update a region snapshot."""
```

**Delete Region Snapshot** (`cli_update.py`)

```python
@update_app.command("delete-region-snapshot")
def delete_region_snapshot(
    snapshot_id: int = typer.Argument(..., help="Snapshot ID"),
    yes: bool = typer.Option(False, "--yes", "-y", help="Skip confirmation"),
):
    """Delete a region snapshot (permanent)."""
```

Similar commands for province snapshots.

### Import/Export Updates

**Export** (`cli_import_export.py`)

Add support for new entity types:
- `--type region-snapshots`
- `--type province-snapshots`

**Import** (`cli_import_export.py`)

Add handlers for importing region/province snapshots from JSON.

---

## Validation Utilities

### New Constants (`utils/validation.py`)

```python
# Snapshot types
VALID_SNAPSHOT_TYPES = [
    "census",
    "simulation",
    "estimate",
    "interpolated",
    "extrapolated",
]

# Temporal granularity
VALID_GRANULARITIES = [
    "year",
    "decade",
    "century",
    "event-driven",
    "custom",
]
```

### New Validation Functions (`utils/validation.py`)

```python
def validate_snapshot_type(snapshot_type: str) -> None:
    """Validate snapshot type."""
    if snapshot_type not in VALID_SNAPSHOT_TYPES:
        raise ValueError(
            f"Invalid snapshot_type: {snapshot_type}. "
            f"Valid types: {', '.join(VALID_SNAPSHOT_TYPES)}"
        )

def validate_granularity(granularity: str) -> None:
    """Validate temporal granularity."""
    if granularity not in VALID_GRANULARITIES:
        raise ValueError(
            f"Invalid granularity: {granularity}. "
            f"Valid granularities: {', '.join(VALID_GRANULARITIES)}"
        )
```

---

## Testing Strategy

### Unit Tests

**Test Interpolation Logic** (`tests/timeline/test_services.py`)

```python
def test_region_snapshot_interpolation_exact_match(db_session):
    """Test interpolation returns exact snapshot if day matches."""
    # Create region and snapshots
    # Query with interpolate flag
    # Assert exact match returned

def test_region_snapshot_interpolation_linear(db_session):
    """Test linear interpolation between two snapshots."""
    # Create snapshots at day 0 (pop 1000) and day 100 (pop 2000)
    # Query day 50
    # Assert population ~1500

def test_region_snapshot_interpolation_species_breakdown(db_session):
    """Test interpolation of species breakdown."""
    # Create snapshots with species data
    # Query intermediate day
    # Assert species percentages interpolated correctly

def test_region_snapshot_nearest_before(db_session):
    """Test nearest-before query logic."""
    # Create snapshots
    # Query with --nearest flag
    # Assert correct snapshot returned

def test_region_snapshot_no_data_available(db_session):
    """Test query when no snapshots exist."""
    # Query non-existent region
    # Assert appropriate error/None
```

**Test Service CRUD** (`tests/timeline/test_services.py`)

```python
def test_create_region_snapshot(db_session):
    """Test creating a region snapshot."""

def test_update_region_snapshot(db_session):
    """Test updating a region snapshot."""

def test_delete_region_snapshot(db_session):
    """Test deleting a region snapshot."""

def test_list_region_snapshots_with_filters(db_session):
    """Test listing snapshots with various filters."""
```

**Test Metadata Validation (ADR-008)** (`tests/timeline/test_models.py`)

```python
def test_region_metadata_flat_structure_allowed(db_session):
    """Test that flat key-value pairs are accepted."""
    region = Region(name="Test", founded_astro_day=0)
    region.merge_metadata({
        "climate_zone": "temperate",
        "importance": "high",
        "rainfall_mm": 450,
        "temp_avg_c": 18.5,
        "needs_review": True
    })
    db_session.add(region)
    db_session.commit()
    assert len(region.meta_data) == 5

def test_region_metadata_nested_rejected(db_session):
    """Test that nested objects are rejected."""
    region = Region(name="Test", founded_astro_day=0)
    with pytest.raises(ValueError, match="Nested objects not allowed"):
        region.merge_metadata({
            "climate": {
                "rainfall": 450,
                "temp": 18.5
            }
        })

def test_region_metadata_array_rejected(db_session):
    """Test that arrays are rejected."""
    region = Region(name="Test", founded_astro_day=0)
    with pytest.raises(ValueError, match="Arrays not allowed"):
        region.merge_metadata({
            "tags": ["important", "review"]
        })

def test_metadata_merge_preserves_existing(db_session):
    """Test merge operation preserves existing keys."""
    region = Region(name="Test", founded_astro_day=0)
    region.merge_metadata({"key1": "value1"})
    region.merge_metadata({"key2": "value2"})
    assert region.meta_data == {"key1": "value1", "key2": "value2"}

def test_metadata_remove_keys(db_session):
    """Test removing specific metadata keys."""
    region = Region(name="Test", founded_astro_day=0)
    region.merge_metadata({"a": 1, "b": 2, "c": 3})
    region.remove_metadata_keys(["b"])
    assert region.meta_data == {"a": 1, "c": 3}

def test_metadata_clear(db_session):
    """Test clearing all metadata."""
    region = Region(name="Test", founded_astro_day=0)
    region.merge_metadata({"key": "value"})
    region.clear_metadata()
    assert region.meta_data is None

def test_description_operations(db_session):
    """Test description mixin operations."""
    region = Region(name="Test", founded_astro_day=0)

    # Update
    region.update_description("Test description")
    assert region.description == "Test description"
    assert region.has_description() is True

    # Clear
    region.clear_description()
    assert region.description is None
    assert region.has_description() is False
```

### Integration Tests

**Test CLI Commands** (`tests/timeline/test_cli_data.py`, `test_cli_list.py`)

```python
def test_add_region_snapshot_cli(test_db, cli_runner):
    """Test CLI command for adding region snapshot."""

def test_list_region_snapshots_interpolate_cli(test_db, cli_runner):
    """Test CLI query with --interpolate flag."""

def test_list_region_snapshots_nearest_cli(test_db, cli_runner):
    """Test CLI query with --nearest flag."""
```

### Test Data Scenarios

**Scenario 1: Sparse Pre-Settlement Data**
- Region "Proto-Northern Lands" (day -10000 to day 0)
- Snapshots at: -10000, -7500, -5000, -2500, 0 (every 2500 days)
- Granularity: "century"
- Query day -3000: Should interpolate between -5000 and -2500

**Scenario 2: Dense Settlement-Era Data**
- Province "Fatunik" (day 0 to day 1000)
- Snapshots every 100 days
- Granularity: "decade"
- Query day 250: Should interpolate between day 200 and day 300

**Scenario 3: Mixed Granularity**
- Region with century-level snapshots (-5000 to -1000)
- Province with decade-level snapshots (-1000 to 0)
- Settlement with year-level snapshots (0 to 500)
- Test queries across era boundaries

---

## File Structure After PR-003a

```
app_timeline/
  __init__.py
  cli.py
  cli_data.py              # UPDATED - new snapshot commands
  cli_list.py              # UPDATED - new snapshot query commands
  cli_update.py            # UPDATED - new snapshot update/delete commands
  cli_import_export.py     # UPDATED - support for new entity types
  config.py
  models/
    __init__.py            # UPDATED - export new models
    base.py                # UPDATED - add DescriptionMixin, MetadataMixin (ADR-008)
    entity.py
    epoch.py               # UPDATED - add description field, DescriptionMixin
    event.py
    province.py            # UPDATED - add description, meta_data, mixins, snapshots relationship
    route.py
    settlement.py          # UPDATED - SettlementSnapshot gets snapshot_type, granularity, MetadataMixin
    region_snapshot.py     # NEW
    province_snapshot.py   # NEW
  services/
    __init__.py            # UPDATED - export new services
    base_service.py
    entity_service.py
    epoch_service.py
    event_service.py
    province_service.py
    region_service.py
    route_service.py
    settlement_service.py
    snapshot_service.py
    region_snapshot_service.py   # NEW
    province_snapshot_service.py # NEW
  utils/
    __init__.py
    temporal.py
    validation.py          # UPDATED - new validation functions
  db/
    __init__.py
    connection.py
    schema.py              # UPDATED - add new tables to validation

tests/timeline/
  conftest.py
  test_config.py
  test_db.py
  test_models.py           # UPDATED - tests for new models
  test_services.py         # UPDATED - tests for new services, interpolation
  test_cli_data.py         # UPDATED - tests for new data commands
  test_cli_list.py         # UPDATED - tests for new list commands
  test_cli_update.py       # UPDATED - tests for snapshot update/delete
  test_cli_import_export.py # UPDATED - tests for new entity types
  test_validation.py       # UPDATED - tests for new validation functions
  test_temporal.py
```

---

## Documentation Updates

### README.md Updates

**New Section: Macro-Scale Demographics**

```markdown
## Macro-Scale Demographics

The timeline system supports demographic tracking at multiple scales:

- **Regional snapshots**: Track population across large geographic areas (e.g., "Proto-Northern Lands")
- **Provincial snapshots**: Track population for administrative divisions (e.g., "Proto-Fatunik")
- **Settlement snapshots**: Track population for specific settlements (existing functionality)

This enables simulation of pre-settlement eras and large-scale demographic trends.

### Creating Snapshots

```bash
# Add a regional snapshot
saskan-timeline data add-region-snapshot --region 1 --day 0 --population 50000 \
  --type census --granularity century

# Add a provincial snapshot
saskan-timeline data add-province-snapshot --province 1 --day 100 --population 10000 \
  --type simulation --granularity decade
```

### Querying with Interpolation

```bash
# Get exact or nearest snapshot
saskan-timeline list region-snapshots --region 1 --day 5000 --nearest

# Get interpolated value
saskan-timeline list region-snapshots --region 1 --day 5000 --interpolate
```

### Snapshot Provenance

Each snapshot tracks how it was created:
- `census`: Authoritative historical record
- `simulation`: Generated by simulation engine
- `estimate`: User-entered educated guess
- `interpolated`: Computed from surrounding snapshots (query-time only)
- `extrapolated`: Projected from known data

### Temporal Granularity

Snapshots can represent different time resolutions:
- `year`: Annual resolution
- `decade`: ~10-turn intervals
- `century`: ~100-turn intervals
- `event-driven`: Tied to specific events
- `custom`: User-defined
```

### TECHNICAL.md Updates

**Add Section: Regional and Provincial Snapshots**

Document:
- Schema for new tables
- Interpolation algorithm details
- Query behavior with sparse data
- Service layer API
- CLI command reference

---

## Success Criteria

1. **Schema Migration:** Database successfully upgraded from v0.2.0 to v0.3.0
2. **Model Creation:** RegionSnapshot and ProvinceSnapshot models work correctly
3. **Service Layer:** CRUD operations for region/province snapshots functional
4. **Interpolation:** Linear interpolation works for population_total and species/habitat breakdowns
5. **CLI Commands:** All new commands work and provide helpful output
6. **Import/Export:** New entity types can be exported and imported via JSON
7. **Tests:** All tests pass (target: 95%+ coverage for new code)
8. **Documentation:** README and TECHNICAL docs updated with examples
9. **Backward Compatibility:** Existing regions/provinces/epochs work with new nullable fields
10. **Query Performance:** Interpolation queries complete in < 100ms for typical datasets

---

## Implementation Order Recommendation

**Phase 1: Schema and Models (Foundation)**
1. Update Region, Province, Epoch models (add fields)
2. Create RegionSnapshot, ProvinceSnapshot models
3. Update schema.py validation
4. Migration tooling (db drop/init for now)
5. Tests for models

**Phase 2: Service Layer**
6. RegionSnapshotService (CRUD + interpolation)
7. ProvinceSnapshotService (CRUD + interpolation)
8. Tests for services (unit tests for interpolation logic)

**Phase 3: CLI Commands**
9. `data add-region-snapshot`, `data add-province-snapshot`
10. `list region-snapshots`, `list province-snapshots` (with --nearest and --interpolate)
11. `update region-snapshot`, `update province-snapshot`
12. `update delete-region-snapshot`, `update delete-province-snapshot`
13. Tests for CLI commands

**Phase 4: Import/Export**
14. Export support for region/province snapshots
15. Import support for region/province snapshots
16. Tests for import/export

**Phase 5: Documentation and Polish**
17. Update README.md
18. Update TECHNICAL.md
19. Update validation.py with new constants
20. Final integration tests
21. Version bump to 0.3.0

---

## Dependencies

### No New Python Packages Required

Using existing stack:
- SQLAlchemy (ORM, schema)
- Typer (CLI)
- Rich (formatting)
- pytest (testing)

### Configuration Updates

**Update `config/timeline/settings.yaml`:**

```yaml
# Add new valid values
snapshot_types:
  - census
  - simulation
  - estimate
  - interpolated
  - extrapolated

granularities:
  - year
  - decade
  - century
  - event-driven
  - custom

# Version
app:
  version: "0.3.0"
```

---

## Version Increment

After PR-003a completion:
- **Version:** 0.3.0 (minor version bump - schema breaking change)
- Update version strings in:
  - `app_timeline/__init__.py`
  - `config/timeline/settings.yaml`
  - `pyproject.toml`
  - Documentation headers

---

## Next Steps After PR-003a

**PR-003b: Macro-Scale Simulation Engine**
- Implement population dynamics formulas (logistic growth, carrying capacity)
- Region/province simulation engine
- Event system basics (migrations, climate events)
- CLI: `saskan-timeline simulate region`, `simulate province`
- Generate snapshots from simulation runs
- Integration with existing snapshot infrastructure

This will leverage the schema and services from PR-003a to run actual simulations.

---

## Risk Assessment

**Low Risk:**
- Schema additions (description, meta_data) are nullable - backward compatible
- New tables don't affect existing functionality
- Service layer follows established patterns
- CLI commands follow existing conventions

**Medium Risk:**
- Interpolation logic complexity (edge cases, data type handling)
- Query performance with large snapshot datasets
- Migration strategy (drop/recreate loses data - must export first)

**High Risk:**
- None identified

**Mitigation Strategies:**
- Comprehensive testing of interpolation edge cases
- Performance testing with large datasets (10,000+ snapshots)
- Clear migration documentation and backup procedures
- Staged rollout (models → services → CLI)

---

## Open Questions - RESOLVED ✅

All questions resolved on 2025-12-30:

1. **Snapshot type values:** ✅ Five types (census, simulation, estimate, interpolated, extrapolated) are sufficient
2. **Granularity values:** ✅ Five granularities (year, decade, century, event-driven, custom) are sufficient
3. **Interpolation for JSON fields:** ✅ Use nearest snapshot (no interpolation for cultural_composition/economic_data)
4. **Export format:** ✅ Only "real" snapshots exported (census/simulation/estimate); analytical engines do their own interpolation
5. **CLI ergonomics:** ✅ `--day` + `--interpolate` pattern approved
6. **Settlement snapshots retrofit:** ✅ YES - Add snapshot_type and granularity fields to settlement_snapshots for consistency

---

## Approval Checklist - COMPLETED ✅

- [x] User approves schema changes
- [x] User approves model design (with ADR-008 mixins)
- [x] User approves service layer (especially interpolation logic)
- [x] User approves CLI command structure
- [x] User answers open questions
- [x] ADR-008 published and integrated
- [x] Migration strategy confirmed (drop/recreate for v0.3.0)

---

**Status:** Approved - Ready for Implementation
